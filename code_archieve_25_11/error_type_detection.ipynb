{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "import numpy\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "#import raha\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_normalizer(value):\n",
    "        \"\"\"\n",
    "        This method takes a value and minimally normalizes it.\n",
    "        \"\"\"\n",
    "        value = html.unescape(value)\n",
    "        value = re.sub(\"[\\t\\n ]+\", \" \", value, re.UNICODE)\n",
    "        value = value.strip(\"\\t\\n \")\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_dataset(dataset_path):\n",
    "        \"\"\"\n",
    "        This method reads a dataset from a csv file path.\n",
    "        \"\"\"\n",
    "        dataframe = pandas.read_csv(dataset_path, sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str,\n",
    "                                    keep_default_na=False, low_memory=False).applymap(value_normalizer)\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = dataset_dictionary[\"name\"]\n",
    "path = dataset_dictionary[\"path\"]\n",
    "dataframe = self.read_csv_dataset(dataset_dictionary[\"path\"])\n",
    "if \"clean_path\" in dataset_dictionary:\n",
    "    has_ground_truth = True\n",
    "    clean_path = dataset_dictionary[\"clean_path\"]\n",
    "    clean_dataframe = read_csv_dataset(dataset_dictionary[\"clean_path\"])\n",
    "if \"repaired_path\" in dataset_dictionary:\n",
    "    has_been_repaired = True\n",
    "    repaired_path = dataset_dictionary[\"repaired_path\"]\n",
    "    repaired_dataframe = read_csv_dataset(dataset_dictionary[\"repaired_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONSTRAINTS = {\n",
    "            \"hospital\": {\n",
    "                \"functions\": [[\"city\", \"zip\"], [\"city\", \"county\"], [\"zip\", \"city\"], [\"zip\", \"state\"], [\"zip\", \"county\"],\n",
    "                              [\"county\", \"state\"]],\n",
    "                \"patterns\": [[\"index\", \"^[\\d]+$\", \"ONM\"], [\"provider_number\", \"^[\\d]+$\", \"ONM\"],\n",
    "                             [\"zip\", \"^[\\d]{5}$\", \"ONM\"], [\"state\", \"^[a-z]{2}$\", \"ONM\"], [\"phone\", \"^[\\d]+$\", \"ONM\"]]\n",
    "            },\n",
    "            \"flights\": {\n",
    "                \"functions\": [[\"flight\", \"act_dep_time\"], [\"flight\", \"sched_arr_time\"], [\"flight\", \"act_arr_time\"],\n",
    "                              [\"flight\", \"sched_dep_time\"]],\n",
    "                \"patterns\": []\n",
    "            },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cfe2a1fab90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset_dictionary = {\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dirty.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"clean_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name = \"flights\"\n",
    "dataset_dictionary = {\n",
    "        \"name\": dataset_name,\n",
    "        \"path\": os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, \"datasets\", dataset_name, \"dirty.csv\")),\n",
    "        \"clean_path\": os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, \"datasets\", dataset_name, \"clean.csv\"))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dboost(dd):\n",
    "        \"\"\"\n",
    "        This method runs dBoost.\n",
    "        \"\"\"\n",
    "        if self.VERBOSE:\n",
    "            print(\"------------------------------------------------------------------------\\n\"\n",
    "                  \"------------------------------Running dBoost----------------------------\\n\"\n",
    "                  \"------------------------------------------------------------------------\")\n",
    "        d = raha.dataset.Dataset(dd)\n",
    "        sp_folder_path = os.path.join(os.path.dirname(dd[\"path\"]), \"raha-baran-results-\" + d.name, \"strategy-profiling\")\n",
    "        strategy_profiles_list = [pickle.load(open(os.path.join(sp_folder_path, strategy_file), \"rb\"))\n",
    "                                  for strategy_file in os.listdir(sp_folder_path)]\n",
    "        random_tuples_list = [i for i in random.sample(range(d.dataframe.shape[0]), d.dataframe.shape[0])]\n",
    "        labeled_tuples = {i: 1 for i in random_tuples_list[:int(d.dataframe.shape[0] / 100.0)]}\n",
    "        best_f1 = -1.0\n",
    "        best_strategy = \"\"\n",
    "        detection_dictionary = {}\n",
    "        for strategy_profile in strategy_profiles_list:\n",
    "            algorithm = json.loads(strategy_profile[\"name\"])[0]\n",
    "            if algorithm == \"OD\":\n",
    "                strategy_output = {cell: \"JUST A DUUMY VALUE\" for cell in strategy_profile[\"output\"]}\n",
    "                er = d.get_data_cleaning_evaluation(strategy_output, sampled_rows_dictionary=labeled_tuples)[:3]\n",
    "                if er[2] > best_f1:\n",
    "                    best_f1 = er[2]\n",
    "                    best_strategy = strategy_profile[\"name\"]\n",
    "                    detection_dictionary = dict(strategy_output)\n",
    "        return detection_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
